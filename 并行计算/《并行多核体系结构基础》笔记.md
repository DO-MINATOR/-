### 一、多核体系结构概述

#### 指令级并行

在多核时代到来之前，倾向于提升晶体管集成度，通过集成，可以将更多的原本片外的部件集成到片内中，从而开发出了诸如流水线、乱序执行、动态分支预测和缓存等，这些“低挂果”虽然硬件实现较为复杂，但生产之后的成本相对较低，开发人员无需考虑数据竞争、同步、通信等问题。

- 逻辑复杂度：例如超标量处理器中大部分部件都用于确保指令的分隔执行。
- 功耗问题：功耗和电容C、电压V^2^和时钟频率f相关，C和集成度相关，V如果降低，将导致f变慢。

#### 数据级并行

由于之前的问题导致集成度无法再提高，频率无法再提升，产生了多核概念。

并行计算机分类：

- SISD，串行体系结构通用模型，但可以实现指令级并行
- SIMD，多用于向量处理器结构
- MISD，多个计算单元依次对同一个数据进行处理，受限于工作模式、应用场景
- MIMD，并行计算机常用结构

MIMD常用体系结构

1、共享高速缓存	2、共享存储器	3、存储器互连	4、分布式集群

受限问题：

- 并行编程复杂度，对数据结构有要求
- 多核间通信导致额外开销，边际效应甚至负增长
- 外围复杂度提升，如“带宽墙”，同样也存在“功耗墙”
- Amdahl定律，解释了串行部分s成为限制加速比的最大因素，且由于多核的设计，也会额外增加开销到s上

### 二、并行编程概述

$$
Speedup_p=\lim_{p\rightarrow+\infin}\frac{1}{s+\frac{1-s}{p}}
$$

s为串行部分(不可并行化)，p为核数，当s=0.01时，100个核也只能提升50倍，同时并形程序额外的开销也将计入s中 ，特别的，当把这一部分开销代入上述公式时，加速比逐渐出现边际效应，甚至是负增长。

#### 共享存储和消息传递

|            | 共享存储               | 消息传递 |
| ---------- | ---------------------- | -------- |
| 通信       | 隐式                   | 显示     |
| 同步       | 显示                   | 隐式     |
| 硬件支持   | 需要                   | 不需要   |
| 编程工作量 | 开始较小，后期快速增长 | 稳定     |
| 调优工作量 | 较高                   | 较低     |
| 通信粒度   | 细                     | 粗       |

#### 其他编程模型

##### 分区化全局地址空间（PGAS）

由于共享存储编程简单，但是后期的调优着重考量于内存中数据的分布，使得开发过程中难度提升。由此提出了分区局部化概念，即在数据的准备工作阶段（计算还未开始），就考虑如何安排数据分布。PGAS是这样一种模型：虽然仍是基于共享存储模型，但不同的线程有自己独有的内存访问空间，即数据亲和力，如矩阵相乘中：
$$
A_{N*P}*B_{P*M}=C_{N*M}
$$
在数据划分阶段时，考虑将A矩阵按行拆分，并且分配给不同线程，即线程i对矩阵A中第i行元素亲和力最高，B由于每个线程都会用到，因此全局共享，不过有一个优化点，就是将B矩阵按照列存储，C矩阵同样按照A那样进行拆分。

由此可见，相比共享存储型，它在数据准备阶段操作较高，但运行时优化性较好。

##### 数据并行编程模型

即同一指令针对一组向量数据进行操作，或者将多对数据拼装成向量形式。在GPU中，实现形式是SIMT，多个核通过锁步的形式执行相同的指令，针对不同数据进行运算。

##### MapReduce模型

通过key-value的形式组织数据存储，读写，map过程中，多个线程输入不同的数据，实现数据并行，而在Reduce中，每个线程处理独自的key，实现了并行规约，自始至终，不会产生线程冲突，只需要类似barrier内存栅栏保证数据完整。由于抽象了数据分布，因此在实现效能上，取决于key-value结构，甚至是内容，这相比PGAS要低，不过灵活性更高。

##### 事务内存TM

允许将一段代码或操作定义为内存中的一个事务，需要保证原子性、隔离性，通常之前保证数据的一致性做法是加锁保护，但这开销较大，且频繁的唤醒和阻塞也会占用cpu性能，而TM基于乐观锁的机制，即事务执行过程中不检查是否冲突，只有在最终提交事务的时候才会检查，有HTM和STM两种实现方式。

### 三、共享存储并行编程

#### 依赖分析

真依赖：S1——>^T^S2，S2依赖于S1产生的数据

反依赖：S1——>^A^S2，S1依赖于S2产生的数据

输出依赖：S1——>^O^S2，S1和S2输出数据到同一位置

后两种依赖通过重构代码可以消除依赖。

循环独立依赖可以做到并行化，而循环传递依赖无法进行并行化，只能由单个线程串行执行。分析依赖关系通过画ITG（循环迭代路径）和LDG（依赖描述图）

#### 并行种类

DOALL：循环独立依赖，循环间不会有依赖，因此每个循环都可以分配到不同线程。

DOACROSS：虽然存在循环传递依赖，但如有非依赖部分，则可以单独抽取出来，进行这部分循环的并行化。

循环分发：前面都是数据并行，这是函数并行，当循环体中存在不相关的数据计算时，可以将不相关的数据计算单独抽取出来新开辟一个循环体。（函数并行源于结构，数据并行源于数据，后者并行度较高）

循环分发的流水线执行：通过函数并行，不同循环体之间仍存在前后依赖关系，则可通过分段的方式将循环体间进行流水并行，执行效率稍比循环分发低。

实际上大致可以分为：循环内部并行（1、2）循环体间并行（3、4）                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              

#### 并行实现的分析

通过对程序组织间的依赖关系进行分析，如画LDG图、线程执行时序图，不断去发掘潜在的并行机会。

在2D灰度图像的平滑滤波中，由于人为的定义了访问次序，因此较难以并行化，但是如果不加以访问限制，即任意的访问次序都可以得到平滑处理较好的结果（事实上此应用也正是如此），那么i、j循环任意并行。

#### 变量范围

- 只读，所有任务都不会改变变量值，随意并行
- 读/写不冲突，任务修改的值不会影响其他任务读取，其他任务读取的值也不会是某一任务写入的值，随意并行
- 读/写冲突，任务修改的值会影响其他任务读取的值，并行约束

冲突变量可通过私有化，变成非冲突变量，通过规约所有私有化的变量进行结果汇总，如果冲突变量无法通过私有化，则并行化难以实现。

需要注意，不同的并行实现，会形成不同的冲突（非冲突）变量，通过合理设置并行实现，可能会省去不必要的同步/锁。

#### 任务划分

静态划分：预先将任务划分，减少了运行过程中的通信量和开销

动态划分：维护一个任务队列，线程执行完任务后，从中取出一个任务，更大的开销，但容易保证负载均衡。

混合划分：先将较大任务分配给线程，剩余任务动态分配，注意任务块大小，过大将导致负载不平衡，过小，通信开销较大，且数据局部性容易遭到破坏。

#### 通信开销

$$
CRR=\frac{通信量}{计算量}
$$

通常不同的任务划分，计算量是一致的，但会导致通信量不同，考虑二维矩阵按照块划分、行划分和列划分，其计算量是一致的，但通信量却不同，这与数据在内存中的分布（行存储、列存储）以及缓存块大小相关。

#### 线程到处理器的映射

操作系统的自主调度系统对于一般的不追求卓越并发性能的机器是可接受的，然而并发程序由于存在同步、通信等问题，有时会造成利用率下降。譬如持有锁的线程被调度出去，抑或是内存栅栏组线程中的某一个线程被唤出，这都会导致cpu利用率的降低。因此产生了成组调度的功能。

数据局部性，对于NUMA系统，线程需安排在离数据近的处理器上。

### 四、针对链式数据结构的并行编程

LDS数据结构由于需要遍历当前节点，才能获得下一个节点，因此存在循环传递依赖，难以并行化。

#### 并行化与遍历

例如，在链式数据结构中，将遍历节点串行执行，而将具体的计算子任务单独拿一个线程执行，

#### 针对LDS的操作并行化

当有两个线程同时对同一个LDS结构进行操作，并且其中一个线程执行的是插入/修改/删除等操作，另一个即使是读取，那么也极有可能发生不可串行化。

相比科学计算中，使用prama并行结构方便的管理和执行并行程序，LDS非数值的并行化操作需要对不同的数据结构和算法单独分析并行。

### 五、存储层次结构概述

#### 放置策略

- 直接相联：一个存储块只能放置在特定的行中，简化查找，然而冲突频繁换出
- 全相联：存储块被放置在任意行中，缓存利用率增大，但查找比较过程繁杂
- 组相联：上述二者方法折中，多采用这种

#### 缓存行大小及映射

- 2的幂次：直接取块数的后log~2~N，N为幂次大小。在传统的幂次方去摸运算中，使得缓存组利用率不均。
- 质数：常见的作法，2的幂次后+1，如2048+1，整个块数作为标签进行比较，且取模运算增加复杂度。
- 伪随机：对数据块地址通过位运算获得对应缓存行号（伪随机），整个块数作为标签进行比较。

#### 数据替换策略

当工作集大于缓存行大小时，需要进行缓存替换，采用何种替换算法将极大影响未来的缓存命中率。

- LRU矩阵替换算法：精确，但空间复杂度为N^2^。
- 伪LRU替换：采用类似二叉树的遍历来选择替换，空间复杂度为N-1。

#### 数据写策略

- 写直达：缓存块中数据更改直接写出，以高带宽占用为代价，当检测到块错误时（简单的错误校验），则重新读取
- 写回：每个数据块额外设置1bit脏位，在换出阶段，如果脏位为1，则写回，减少带宽。但由于只有缓存中有真实数据，所以需要更加复杂的校验保护如ECC，同时，还要避免缓存一致性问题。

通常，L1、L2由于都集成在片内，所以带宽较大，因此采用写直达，L2和主存带宽宝贵，因此采用写回策略。

#### 虚拟存储与高速缓存的执行关系

虚拟存储使得在一个系统中可以运行多道程序，每个进程拥有独立的逻辑地址空间，通过页表机制，将逻辑地址映射到物理地址，然而这一技术的引入使得访存次数*2，极大的降低了系统运行效率。通过改善计算物理地址与访问缓存的关系，可以缓解这一问题。

- 物理寻址：传统访问方式，首先通过虚拟页号比对TLB/页表得到物理块地址，与页内偏移量拼接得到物理地址，然后划分块内偏移和块编号，在缓存中访问。
- 混合寻址：由于给出的逻辑地址中的偏移量可以划分为块内偏移和索引，因此在计算物理地址的同时，将索引送到缓存中，这样物理地址计算完后，只需要比较物理块号和缓存标签，即可判断此次缓存是否命中。

#### 非阻塞式高速缓存

通常发生缺页时，cpu会阻塞住，即等待缺失的缓存块从内存中取得，现代cpu中，设置一个MSHR用于暂存该缺失状态，使得cpu可以继续其他工作，例如发生数据写回指令缺失，则将要写回的数据放入到MSHR中，然后执行别的程序，当缺失块被调入缓存中，直接将MSHR写入到缓存块中，读取缺失同理。

### 六、共享存储多处理器简介

#### 缓存一致性问题

由于多核间不同缓存中的变量是私有的，尽管之前来源于主存中同一个变量，处理器对于缓存中变量的修改对于另一个核是不可见的，通过volatile去除可见性问题。

#### 存储一致性问题

即使没有缓存一致性带来的问题，多核之间也会存在存储顺序不一致带来的存储一致性问题，其原因在于指令顺序的重排序（为了提高指令级并行度），通过volatile去除重排序。

#### 同步问题

指令的原子级执行，即保证一组操作得到硬件上同步的支持。

### 七、缓存一致性基础

