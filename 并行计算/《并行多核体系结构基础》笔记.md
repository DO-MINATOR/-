### 一、多核体系结构概述

#### 指令级并行

在多核时代到来之前，倾向于提升晶体管集成度，通过集成，可以将更多的原本片外的部件集成到片内中，从而开发出了诸如流水线、乱序执行、动态分支预测和缓存等，这些“低挂果”虽然硬件实现较为复杂，但生产之后的成本相对较低，开发人员无需考虑数据竞争、同步、通信等问题。

- 逻辑复杂度：例如超标量处理器中大部分部件都用于确保指令的分隔执行。
- 功耗问题：功耗和电容C、电压V^2^和时钟频率f相关，C和集成度相关，V如果降低，将导致f变慢。

#### 数据级并行

由于之前的问题导致集成度无法再提高，频率无法再提升，产生了多核概念。

并行计算机分类：

- SISD，串行体系结构通用模型，但可以实现指令级并行
- SIMD，多用于向量处理器结构
- MISD，多个计算单元依次对同一个数据进行处理，受限于工作模式、应用场景
- MIMD，并行计算机常用结构

MIMD常用体系结构

1、共享高速缓存	2、共享存储器	3、存储器互连	4、分布式集群

受限问题：

- 并行编程复杂度，对数据结构有要求
- 多核间通信导致额外开销，边际效应甚至负增长
- 外围复杂度提升，如“带宽墙”，同样也存在“功耗墙”
- Amdahl定律，解释了串行部分s成为限制加速比的最大因素，且由于多核的设计，也会额外增加开销到s上

### 二、并行编程概述

$$
Speedup_p=\lim_{p\rightarrow+\infin}\frac{1}{s+\frac{1-s}{p}}
$$

s为串行部分(不可并行化)，p为核数，当s=0.01时，100个核也只能提升50倍，同时并形程序额外的开销也将计入s中 ，特别的，当把这一部分开销代入上述公式时，加速比逐渐出现边际效应，甚至是负增长。

#### 共享存储和消息传递

|            | 共享存储               | 消息传递 |
| ---------- | ---------------------- | -------- |
| 通信       | 隐式                   | 显示     |
| 同步       | 显示                   | 隐式     |
| 硬件支持   | 需要                   | 不需要   |
| 编程工作量 | 开始较小，后期快速增长 | 稳定     |
| 调优工作量 | 较高                   | 较低     |
| 通信粒度   | 细                     | 粗       |

#### 其他编程模型

##### 分区化全局地址空间（PGAS）

由于共享存储编程简单，但是后期的调优着重考量于内存中数据的分布，使得开发过程中难度提升。由此提出了分区局部化概念，即在数据的准备工作阶段（计算还未开始），就考虑如何安排数据分布。PGAS是这样一种模型：虽然仍是基于共享存储模型，但不同的线程有自己独有的内存访问空间，即数据亲和力，如矩阵相乘中：
$$
A_{N*P}*B_{P*M}=C_{N*M}
$$
在数据划分阶段时，考虑将A矩阵按行拆分，并且分配给不同线程，即线程i对矩阵A中第i行元素亲和力最高，B由于每个线程都会用到，因此全局共享，不过有一个优化点，就是将B矩阵按照列存储，C矩阵同样按照A那样进行拆分。

由此可见，相比共享存储型，它在数据准备阶段操作较高，但运行时优化性较好。

##### 数据并行编程模型

即同一指令针对一组向量数据进行操作，或者将多对数据拼装成向量形式。在GPU中，实现形式是SIMT，多个核通过锁步的形式执行相同的指令，针对不同数据进行运算。

##### MapReduce模型

通过key-value的形式组织数据存储，读写，map过程中，多个线程输入不同的数据，实现数据并行，而在Reduce中，每个线程处理独自的key，实现了并行规约，自始至终，不会产生线程冲突，只需要类似barrier内存栅栏保证数据完整。由于抽象了数据分布，因此在实现效能上，取决于key-value结构，甚至是内容，这相比PGAS要低，不过灵活性更高。

##### 事务内存TM

允许将一段代码或操作定义为内存中的一个事务，需要保证原子性、隔离性，通常之前保证数据的一致性做法是加锁保护，但这开销较大，且频繁的唤醒和阻塞也会占用cpu性能，而TM基于乐观锁的机制，即事务执行过程中不检查是否冲突，只有在最终提交事务的时候才会检查，有HTM和STM两种实现方式。

### 三、共享存储并行编程

#### 依赖分析

真依赖：S1——>^T^S2，S2依赖于S1产生的数据

反依赖：S1——>^A^S2，S1依赖于S2产生的数据

输出依赖：S1——>^O^S2，S1和S2输出数据到同一位置

后两种依赖通过重构代码可以消除依赖。

循环独立依赖可以做到并行化，而循环传递依赖无法进行并行化，只能由单个线程串行执行。分析依赖关系通过画ITG（循环迭代路径）和LDG（依赖描述图）

#### 并行种类

DOALL：循环独立依赖，循环间不会有依赖，因此每个循环都可以分配到不同线程。

DOACROSS：虽然存在循环传递依赖，但如有非依赖部分，则可以单独抽取出来，进行这部分循环的并行化。

循环分发：前面都是数据并行，这是函数并行，当循环体中存在不相关的数据计算时，可以将不相关的数据计算单独抽取出来新开辟一个循环体。（函数并行源于结构，数据并行源于数据，后者并行度较高）

循环分发的流水线执行：通过函数并行，不同循环体之间仍存在前后依赖关系，则可通过分段的方式将循环体间进行流水并行，执行效率稍比循环分发低。

实际上大致可以分为：循环内部并行（1、2）循环体间并行（3、4）                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              

#### 并行实现的分析

通过对程序组织间的依赖关系进行分析，如画LDG图、线程执行时序图，不断去发掘潜在的并行机会。

在2D灰度图像的平滑滤波中，由于人为的定义了访问次序，因此较难以并行化，但是如果不加以访问限制，即任意的访问次序都可以得到平滑处理较好的结果（事实上此应用也正是如此），那么i、j循环任意并行。

#### 变量范围

- 只读，所有任务都不会改变变量值，随意并行
- 读/写不冲突，任务修改的值不会影响其他任务读取，其他任务读取的值也不会是某一任务写入的值，随意并行
- 读/写冲突，任务修改的值会影响其他任务读取的值，并行约束

冲突变量可通过私有化，变成非冲突变量，通过规约所有私有化的变量进行结果汇总，如果冲突变量无法通过私有化，则并行化难以实现。

需要注意，不同的并行实现，会形成不同的冲突（非冲突）变量，通过合理设置并行实现，可能会省去不必要的同步/锁。

#### 任务划分

静态划分：预先将任务划分，减少了运行过程中的通信量和开销

动态划分：维护一个任务队列，线程执行完任务后，从中取出一个任务，更大的开销，但容易保证负载均衡。

混合划分：先将较大任务分配给线程，剩余任务动态分配，注意任务块大小，过大将导致负载不平衡，过小，通信开销较大，且数据局部性容易遭到破坏。

#### 通信开销

$$
CRR=\frac{通信量}{计算量}
$$

通常不同的任务划分，计算量是一致的，但会导致通信量不同，考虑二维矩阵按照块划分、行划分和列划分，其计算量是一致的，但通信量却不同，这与数据在内存中的分布（行存储、列存储）以及缓存块大小相关。

#### 线程到处理器的映射

操作系统的自主调度系统对于一般的不追求卓越并发性能的机器是可接受的，然而并发程序由于存在同步、通信等问题，有时会造成利用率下降。譬如持有锁的线程被调度出去，抑或是内存栅栏组线程中的某一个线程被唤出，这都会导致cpu利用率的降低。因此产生了成组调度的功能。

数据局部性，对于NUMA系统，线程需安排在离数据近的处理器上。

更一般的

