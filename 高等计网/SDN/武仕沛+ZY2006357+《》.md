### 材料搜集

Google选择使用SDN来改造数据中心之间互联的WAN网（即G-scale Network），因为这个网络相对简单，设备类型以及功能比较单一，而且WAN网链路成本高昂（比如很多海底光缆），所以对WAN网的改造无论建设成本、运营成本收益都非常显著。

以往的google网络使用静态hash实现负载均衡，但难以做到真正的复杂均衡，且无法针对不同的应用区别对待，因此急于增加网络利用率、简化管理。流量工程、软件控制。

#### 改进过程：

- 第一阶段在2010年春天完成，把OpenFlow交换机引入到网络里面，但这时OpenFlow交换机对同网络中的其他非OpenFlow设备表现得就像是传统交换机一样，只是网络协议都是在Controller上完成的，外部行为来看表现得仍然像传统网络。
- 第二阶段是到 2011年中完成，这个阶段引入更多流量到OpenFlow网络中，并且开始引入SDN管理，让网络开始向SDN网络演变。
- 第三个阶段在2012年初完 成，整个B4网络完全切换到了OpenFlow网络，引入了流量工程，完全靠OpenFlow来规划流量路径，对网络流量进行极大的优化。

#### 架构：

第一层物理交换机，采用NDM（可协商的数据转发面模型），允许厂商基于实际的应用需求和现有的芯片架构来定义很多种不同的转发模型，每种模型可以涉及到多张表，匹配不 同的字段，基于查找结果执行不同的动作。由于是基于现有的芯片，所以无论匹配的字段还是执行的动作都是有限制的，不能随心所欲。

第二层controller，按照功能进行选举，也就是说 对于控制功能A，可能选举Controller1为leader；而对于控制功能B，则有可能选举Controller2为leader。这里说的leader就是OpenFlow标准里面的master。多个controller被controller-logic进行统一管理，收集本site信息，然后计算出路由。

第三层全局控制器，作为TE server收集所有site的全局信息。当一个新的业务要运行时，TE server为其计算一条最佳路由FG，从而整体上使链路带宽 利用率达到最优。SDN Gateway作为中间件进行翻译。

#### 研究重点：

带宽分配、路径计算。基于特定应用程序分配带宽，基于｛源数 据中心，目的数据中心，QoS｝来维护流表项，多个不同的应用QoS相同，则认为是同一类应用，并且如果出入节点都相同，则认为是同一条流。TE server根据controller收集的信息，然后输入到bandwidth函数中，最终输出结果，得到分配的带宽大小。在一切计算完后，将配置信息下发到controller，再下发到交换机中。

另外，架构中同时存在下发的ACL控制表，和传统的LPM路由表，前者优先级更高，扩展更灵活，可以随时改为传统网络。另外，内部应用程序产生的流肯定是通过ACL才能达到最优，而外部进来的报文或者中转的报文也都是通过LPM转发表转发的。

### 0摘要

介绍B4的设计、概念、实现，他是一个链接谷歌数据中心的的私有广域网。特点：少量站点、大量带宽，寻求最大化弹性流量需求，对网络边缘的完全控制。使用OpenFlow交换机提高链路利用率到100%。推广B4的经验

### 1介绍

广域网通常将可靠性过度配置，屏蔽了网络设备的故障，然后这样做的成本是高昂的。通过SDN，最大化B4的应用需求，使得部署新的路由、调度、监视和管理功能更加简单，形成一个高效的网络。

具体做法：

1. 通过网络边缘控制在资源竞争期间进行裁决
2. 使用多路径转发/隧道技术，并根据程序优先级平衡网络可用容量
3. 故障或需求增大时，动态分配带宽

综上，平均利用率达到70%，（个别100%）

### 2背景介绍

谷歌网络面向两个网络，一个是面向用户的、支持广泛协议的、拓扑密集的网络，支持高度可用性。

B4网络，用户数据存储、远程存储访问（计算资源、存储资源分离）、B4间的数据同步，按照体量大小、延迟敏感度、优先级排序。

数据中心的需求：

1. 弹性带宽需求。
2. 较少的数据站点
3. 通过其上的软件进行网络配置
4. 成本更加敏感

### 3设计

![image-20201027133239565](https://imagebag.oss-cn-chengdu.aliyuncs.com/img/image-20201027133239565.png)

- 交换机硬件是 Google 定制的，负责转发流量，不运行复杂的控制软件。
- OpenFlow Controller (OFC) 根据网络控制应用的指令和交换机事件，维护网络状态。
- Central TE Server 是整个网络逻辑上的中心控制器。
- 第一条虚线上面是 Central **TE** (Traffic Engineering) Server。
- 一二两条虚线之间是每个数据中心（Site）的控制器，被称为 Network Control Server (**NCS**)，其上运行着 OpenFlow Controller (**OFC**) 集群，使用 Paxos 协议选出一个 master，其他都是热备（hot standby）。
- 二三两条虚线之间是交换机（switch），运行着 OpenFlow Agent (**OFA**)，接受 OFC 的指令并将 TE 规则写到硬件 flow-table 里。

#### 交换机

通常广域网的交换机具有大容量缓存，大的转发表。Google通过设置合理的flow groups，避免缓存占用，以及大规模转发表。通过现有分布式技术提高软件容错性。通过定制硬件获得更佳的性能优势。部署多端口，减少硬件数量，带来可控的SDN模型。问题：openflow协议下发的流表需要同不同版本的控制器兼容，硬件底层实现机制不同。

#### 路由

![B4-03](https://imagebag.oss-cn-chengdu.aliyuncs.com/img/google B4-03.png)

RIB（Routing Information Base）保存网络拓扑，路由规则，结合ECMP实现负载均衡。

**Quagga**: Google 采用的开源 BGP/ISIS 协议，以实现传统网络和SDN网络的结合。

RAP（Routing Application Proxy）负责OFA和OFC的互联。

flows对应match部分，groups代表action部分。

#### 流量工程

使用多条链路为应用程序提供共享带宽，并通过最大最小分配流量

![image-20201027145400692](https://imagebag.oss-cn-chengdu.aliyuncs.com/img/image-20201027145400692.png)

TE server负责路由的计算：

{Source site, Dest site, Priority}还有一项重要的bandwidth属性需要计算，TE Optimization 的目标是 最大最小公平原则，即在公平的前提下满足更多的需求，如app1需要15G带宽，app2需要5G，app3无上限，那么优先级的分配可以是10、1、0.5。通过这样得到新的FG表项，即{Source site, Dest site, Priority, Bandwidth}。代表一类应用的需求特征，如果这些参数大致一样的话，那么可以认为是同一类应用。以减小流表项的规模，划分出流后，这作为TE optimization algorithm的第一个输入，即FGs。另一个输入是由SDN Gateway收集来的链路信息，通过二者的输入，最终得到分配的隧道。

隧道分配的关键在于所需的带宽大小，以及离散化的粒度大小。

#### 流量分配算法

根据流量需求和优先级分配流量。

1. 选定初始的隧道
2. 为每个隧道按照优先级分配带宽，要么平等对待，要么完全满足。
3. 如果每一个隧道都被分配完，则结束，否则进入4
4. 剥离超过平均满足水平的链路带宽，加入待剩余分配带宽，进入2。

#### 隧道离散化

需要对流量分配算法分得的流量进行离散化，由于硬件支持的流量控制无法无限精细化，因此通过贪心算法求解。

1. 将隧道进行分裂，并对每个分得的流量向下取整。
2. 对每一个单独的分裂的小的隧道，作为因子，分配给相应流
3. 如果分配完毕则结束，否则进入2

离散化会降低性能，由于不能无限分割，总存在浪费带宽资源。分配精细程度越高，利用率越高，但相应的控制成本增加，google最终选定分配粒度为1/4

![image-20201027152834242](https://imagebag.oss-cn-chengdu.aliyuncs.com/img/image-20201027152834242.png)

### 状态描述

状态表征隧道、流组

![image-20201027153628890](https://imagebag.oss-cn-chengdu.aliyuncs.com/img/image-20201027153628890.png)

- Encap Switch 是连接终端机器的边界路由器，它们将 IP packet 封装起来，包上路由专用的 source ip 和 destination ip。
- Transit Switch 是中间传输路由器，它们只接受经过 Encap Switch 封装的 IP packet，并在数据中心之间进行路由。
- Decap Switch 是连接终端机器的边界路由器，其实跟 Encap Switch 是同一批机器。它们将被封装的 IP packet 剥掉一层皮，送给终端机器。

在控制层面，google将TE和传统路由结合运行，TE优先级较高，这样SDN就可以部署到数据中心，且SDN如果出问题，则随时可以切换为传统网络。

![image-20201027154304795](https://imagebag.oss-cn-chengdu.aliyuncs.com/img/image-20201027154304795.png)

具体的，每个交换机都有两张表，一个LPM表，由传统的路由协议算法维护，还有个ACL表，通过TE计算得来，优先级更高。计算出的路径最终都要通过hash算法选择到具体某一路由，以实现负载均衡。

### 顺序依赖

TE的变更会导致转发平面数据包的丢失（如果有正在传输的数据），因此增加两条规则：

1. 修改流表项之前，在受影响的数据中心间建立好专用tunnel
2. 按照优先级设定，逐次进行条目的变动。

### 部署效果

![image-20201027155447946](https://imagebag.oss-cn-chengdu.aliyuncs.com/img/image-20201027155447946.png)

这是B4的流量增长示意图，可以看出SDN带来的改进是十分明显的。此外，还可以快速测试新的功能。另外，即使采用了主从controller机制，稳定性依然很强，主节点通常可以保持11天的工作状态。通过路由聚合，显著降低了系统负载和链路颠簸。

### 作用

1. 第一个公开的成功的SDN案例，让人了解到分布式的controller应当如何工作及工作效果。
2. QoS四元组的制定推进了流量工程，将应用特性作为制定转发路径时考虑的因素是一个不错的尝试。
3. 演示了如何将SDN和传统路由算法相结合，为从传统网络过渡到SDN提供了经验。